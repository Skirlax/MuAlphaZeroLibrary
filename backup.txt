pis, vs, rews_moves_players, states = zip(*experience_batch)
rews = [x[0] for x in rews_moves_players]
moves = [x[1] for x in rews_moves_players]
states = [np.array(x) for x in states]
states = th.tensor(np.array(states), dtype=th.float32, device=device).permute(0, 3, 1, 2)
pis = [list(x.values()) for x in pis]
pis = th.tensor(np.array(pis), dtype=th.float32, device=device)
vs = th.tensor(np.array(vs), dtype=th.float32, device=device).unsqueeze(0)
rews = th.tensor(np.array(rews), dtype=th.float32, device=device).unsqueeze(0)
latent = self.representation_forward(states)
pred_pis, pred_vs = self.prediction_forward(latent)
# masks = mask_invalid_actions_batch(self.game_manager.get_invalid_actions, pis, players)
latent = match_action_with_obs_batch(latent, moves)
_, pred_rews = self.dynamics_forward(latent)
priorities = priorities.to(device)
balance_term = muzero_config.balance_term
if muzero_config.enable_per:
    w = (1 / (len(priorities) * priorities)) ** muzero_config.beta
    w /= w.sum()
    w = w.reshape(pred_vs.shape)
else:
    w = 1
loss_v = mse_loss(pred_vs, vs) * balance_term * w
loss_pi = self.muzero_pi_loss(pred_pis, pis) * balance_term * w
loss_r = mse_loss(pred_rews, rews) * balance_term * w
loss = loss_v.sum() + loss_pi.sum() + loss_r.sum()
return loss, loss_v.sum(), loss_pi.sum(), loss_r.sum()